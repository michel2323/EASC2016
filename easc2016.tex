% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012
\newcommand{\refalg}[1]{Algorithm~\ref{#1}}
\newcommand{\refsec}[1]{Sect.~\ref{#1}}
\newcommand{\reffig}[1]{Fig.~\ref{#1}}
\newcommand{\refsubfig}[1]{Fig.~\subref{#1}}
\newcommand{\reftab}[1]{Table~\ref{#1}}
\newcommand{\refeqn}[1]{(\ref{#1})}
\newcommand{\reflst}[1]{Listing~(\ref{#1})}
\newcommand{\bigo}[1]{\mathcal{O}(#1)}

\documentclass{sig-alternate}

\usepackage{subfigure}
\usepackage{enumitem}

\begin{document}

% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\doi{000000} ??????  

% ISBN
%\isbn{000000} ???????

%Conference
\conferenceinfo{EASC '16}{April 26--29, 2016, Stockholm, Sweden}

%\acmPrice{\$15.00}

%
% --- Author Metadata here ---
\conferenceinfo{EASC2016}{'16, Stockholm, Sweden}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{On the Strong Scaling of the Spectral Element Solver Nek5000 on Petascale Systems}
%\subtitle{Subtitle}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{10} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
% 1st. author
\alignauthor
Nicolas Offermans\\
       \affaddr{Linn\'{e} Flow Center}\\
       \affaddr{KTH Mechanics, Royal Institute of Technology}\\
       \affaddr{10044 Stockholm, Sweden}\\
       \email{nof@mech.kth.se}
% 2nd. author
\alignauthor
Oana Marin\\
       \affaddr{Mathematics and Computer Science Division}\\
       \affaddr{Argonne National Laboratory}\\
       \affaddr{Argonne, IL, USA}\\
       \email{oanam@mcs.anl.gov}
% 3rd. author
\alignauthor 
Michel Schanen\\
       \affaddr{Mathematics and Computer Science Division}\\
       \affaddr{Argonne National Laboratory}\\
       \affaddr{Argonne, IL, USA}\\
       \email{mschanen@anl.gov}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor
Jing Gong\\
       \affaddr{PDC-HPC}\\
       \affaddr{KTH, Royal Institute of Technology}\\
       \affaddr{10044 Stockholm, Sweden}\\
       \email{gongjing@kth.se}
\alignauthor 
Paul Fischer\\
       \affaddr{Siebel Center for Computer Science}\\
       \affaddr{University of Illinois at Urbana-Champaign}\\
       \affaddr{Urbana, IL, USA}\\
       \email{fischerp@illinois.edu}
% 5th. author
\alignauthor 
Philipp Schlatter\\
       \affaddr{Linn\'{e} Flow Center}\\
       \affaddr{KTH Mechanics, Royal Institute of Technology}\\
       \affaddr{10044 Stockholm, Sweden}\\
       \email{pschlatt@mech.kth.se}
% 6th. author
}
\additionalauthors{Additional authors: Adam Peplinski (KTH,
email: {\texttt{adam@mech.kth.se}}) and Elia Merzari
(ANL, email: {\texttt{emerzari@anl.gov}})
and Aleks Obabko
(ANL, email: {\texttt{obabko@mcs.anl.gov}})
and Maxwell Hutchinson
(University of Chicago, email: {\texttt{maxhutch@uchicago.edu}}).}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{21 March 2016}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
The present work is targeted at performing a strong scaling study of the high-order spectral element fluid dynamics solver Nek5000.  Prior studies such as \cite{fischer:scaling} indicated a recommendable metric for strong scalability from a theoretical viewpoint, which we test here extensively on three parallel machines with different performance characteristics. The test cases considered for the simulations correspond to a turbulent flow in a straight pipe at four different friction Reynolds numbers $Re_{\tau} = 180$, $360$, $550$ and $1000$. Results on various machines with various architectures and interconnect networks, namely Mira (IBM Blue Gene/Q), Beskow (Cray XC40) and Titan (Cray XK7) are studied.  Considering the linear model for parallel communication we quantify the machine characteristics in order to better assess the scaling behaviors of the code. Subsequently the main blocks of the code are timed to obtain computation cost while sampling and profiling tools are used to measure the communication time over a large range of compute cores.
We also study the effect of the two coarse grid solvers XXT and AMG on the
computational time. Super linear scaling due to a reduction in cache misses is
observed on each computer. The strong scaling limit is attained for roughly
$5000-10,000$ degrees of freedom per core on Mira, $30,000-50,0000$ on Beskow,
with only a small impact of the problem size for both machines, and ranges
between $10,000$ and $220,000$ depending on the problem size on Titan. This work
aims at being a reference for Nek5000 users and also serve as a basis for
potential issues to address as the community heads towards exascale supercomputers.
\end{abstract}


% Code generated by the tool at
% http://dl.acm.org/ccs.cfm
%
% Is it necessary?
 \begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010169.10010170</concept_id>
<concept_desc>Computing methodologies~Parallel algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010169.10010170.10010174</concept_id>
<concept_desc>Computing methodologies~Massively parallel algorithms</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003777.10003780</concept_id>
<concept_desc>Theory of computation~Communication complexity</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10010405.10010432</concept_id>
<concept_desc>Applied computing~Physical sciences and engineering</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Parallel algorithms}
\ccsdesc[300]{Computing methodologies~Massively parallel algorithms}
\ccsdesc[300]{Theory of computation~Communication complexity}
\ccsdesc[300]{Applied computing~Physical sciences and engineering}


%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

% We no longer use \terms command
%\terms{Theory}

\keywords{Nek5000; Scaling; Benchmarking}

\section{Introduction}
The development of highly scalable codes that perform well on different
architectures has been a daunting task ever since the advent of High Performance
Computing, due to the interplay between computation and communication,
inescapable global operations but foremost due to the nature of this research
field constantly redefining its path. In the current work we explore the parallelism of Nek5000, which is one of the oldest legacy codes (celebrating 30 years this year) and thus has experienced many trends and changes in high performance computing strategies.

Nek5000 is a spectral element, thermal hydraulics code which performs best on
complex geometries, wall bounded problems (although it can handle any type of
boundary condition), at large scales on any commonly used parallel computer
architecture. The present study is aimed at providing users a handle on
parameter choices for performance and scalability, and relies on previous work ,
such as \cite{fischer:scaling} and \cite{tufo:terascale}. Hereby we benchmark
the code on a canonical flow case, a Direct Numerical Simulation case of
incompressible flow in a pipe at increasingly high Reynolds numbers
\cite{Khoury2013}. As commonly known solving the Poisson equation for the
pressure is the most challenging computational part of an incompressible flow
solver. Nek5000 relies on a domain decomposition approach to solving the Poisson
subproblem, where the coarse grid solver is preconditioned either via XXT \cite{Tufo2001151} or AMG (ref report James). We explore both approaches and quantify the regimes in which either of them is recommendable. Subsequently we identify the strong scaling limit on various flow configurations and machines. For a more complete interpretation of the results we assess also load balancing, mesh partitioning, cache misses etc. 

\section{Code description}
Nek5000 supports a wide set of options that speed up the time to solution, such
as the method of characteristics which decouples the pressure solve from the
restrictive CFL condition for the nonlinear advection operator, or orthogonal
projections of the solution to reduce the iteration count of the algebraic
solver etc. Here we focus only one track to solution which is consistent with
the physical case we study and the way it was initially performed, i.e. Direct Numerical Simulation of pipe flow \cite{Khoury2013}.

\subsection{Numerical method}
There are two main solvers available within Nek5000 for computing the solution of incompressible Navier-Stokes, and of these one is also amenable to non-divergence free flows, as available in \cite{Tomboulides1997}. Although we operate in the incompressible regime we picked this solver to preserve generality. The incompressible Navier-Stokes are given here
%As a fractional step solver the following equations
\begin{align} 
 \frac{\partial \mathbf{u}}{\partial t} + (\mathbf{u \cdot \nabla}) \mathbf{u} & = - \nabla p + \frac{1}{Re} \nabla^2 \mathbf{u} + \mathbf{f} \label{eqn:NS_momentum},\\
 \nabla \cdot \mathbf{u} & = 0, \label{eqn:NS_continuity}
\end{align}
where $\mathbf{u}$ is the velocity and $p$ the pressure. The Reynolds number $Re = \frac{U L}{\nu}$ is expressed as a function of a typical velocity scale $U$, length scale $L$ and kinematic viscosity $\nu$. Equations \refeqn{eqn:NS_momentum}  and \refeqn{eqn:NS_continuity} are called the continuity and momentum equations respectively. 

The momentum equation is time integrated via an Implicit-Explicit scheme, also
known as BDFk-EXTk (Backward DiFferences and EXTrapolation of order k). We
illustrate it semi-discretely

\begin{eqnarray}
\sum\limits_{j=0}^k \frac{b_j}{\Delta t} \mathbf u^{n-j}  = - \nabla p^{n}+\frac{1}{Re}\nabla^2\mathbf u^{n}+\underbrace{\sum\limits_{j=1}^k a_j [N(\mathbf u^{n-j})+\mathbf f^{n}]}_{\mathbf{F}_n(\mathbf u,\mathbf f)}.\label{eqn:discrete}
\end{eqnarray}
here we denoted the nonlinear operator $\mathbf u \cdot \nabla \mathbf u=N(\mathbf u)$ and $b_k$, $a_k$ are the coefficients of the implicit time derivative discretization, and explicit extrapolation respectively.

Ignoring boundary conditions and other numerical technicalities available in \cite{Tomboulides1997} we end up solving
\begin{eqnarray}
 \Delta p^{n} = \nabla \cdot \left( -\frac{b_0}{\Delta t} \mathbf{u}^{n} + \mathbf{F}_n \left( \mathbf{u},\mathbf f \right) \right), \label{eqn:hmhz_pres}\\
 \Delta \mathbf{u}^{n+1}- \frac{b_0}{\Delta t} \mathbf{u}^{n+1}  =  \nabla p^{n+1} + \mathbf{F}_n \left( \mathbf{u}, \mathbf f) \right. . \label{eqn:hmhz_vel}
\end{eqnarray}

As it can be observed, solving the incompressible Navier-Stokes equations is reduced to the evaluation of $\mathbf{F}_n$ in \refeqn{eqn:discrete}, followed by one Poisson equation and a Helmholtz equation thereafter. 
Equation \refeqn{eqn:hmhz_pres}, the Poisson equation for the pressure, is the main source of stiffness and its efficient resolution by an iterative solver is preceded by two steps. First of all, the pressure at each time iteration is projected onto a subspace of previous solutions, and as described in \cite{Fischer1998} has been shown to reduce the iteration count by a factor $2.5-5$, which we also verify in Section~\ref{sec:analysis}. Then, a pressure preconditioner is built based on the additive overlapping Schwarz method, given by 
\begin{equation}
 M_0^{-1} := R_0^T A_{0}^{-1} R_0 + \sum_{k=1}^{K} R_k^T \tilde{A}_k^{-1} R_k.
\end{equation}
The overlapping part requires local solves on each subdomain and is easily parallelized \cite{Fischer199784,Fischer2005}. The coarse grid solve is more difficult to parallelize and can be performed in two different ways. The first method is a Cholesky factorization of the matrix $A_0^{-1}$ into $XX^T$ with a convenient refactoring of the underlying matrix to maximize the sparsity pattern of $X^T$. This factorization is subsequently referred to as XXT and details regarding complexity and implementation are available in \cite{Tufo2001151}. 

The second method is an Algebraic Multigrid Solver (AMG) that operates a smoothing, a coarsening and an interpolation operation on a predefined number of coarse grid levels. Finally, the pressure equation is solved with the generalized minimal residual method (GMRES).

In a similar way, Equation \refeqn{eqn:hmhz_vel}, the Helmholtz equation for the velocity, is solved using the conjugate gradient (CG) iterative solver with a simple Jacobi preconditioner.
% \begin{align}
%  \mathbf{u}_i^* & = \beta_0 \mathbf{u}_i^n + \beta_1 \mathbf{u}_i^{n-1} + \beta_2 \mathbf{u}_i^{n-2}, \label{eqn:extrap_vel}\\
%  \mathbf{h}_i^n & = N(\mathbf{u}_i^n) + B \mathbf{f}_i + ???, \label{eqn:rhs}\\
%  A \delta \mathbf{p}^n & = \nabla \cdot \left[ \left( \sum_{i=1}^3 \frac{-1}{Re} (\nabla \times (\nabla \times \mathbf{u}_i^*)) + B^{-1} \mathbf{h}_i^n \right) \right]\nonumber\\
%  & \quad - A \mathbf{p}^n , \label{eqn:hmhz_pres} \\
%  A \delta \mathbf{u}^{n} & = -A \mathbf{u}^{n} + \mathbf{h}_i^n \label{eqn:hmhz_vel},\\
%  \mathbf{p}^{n+1} &= \mathbf{p}^{n} + \mathbf{\delta p}^{n}, \label{eqn:update_p} \\
%  \mathbf{u}_i^{n+1}& = \mathbf{u}_i^{n} + \mathbf{\delta u}_i^{n}.  \label{eqn:update_u} 
% \end{align}
% The operators $A$ and $B$ represent the discrete Laplacian operator and the mass matrix respectively. The index $i=1,2,3$ represents the 3 spatial directions $x$, $y$ and $z$. The intermediate velocity $\mathbf{u}_i^*$ is extrapolated using a third order Adams-Bashforth scheme in Equation (\refeqn{eqn:extrap_vel}). The corresponding coefficients $\beta_k$ are given by $\beta_0 = \frac{23}{12}$, $\beta_1 = \frac{-4}{3}$ and $\beta_2 = \frac{5}{12}$ (??). The term $\mathbf{h}_i^n$ from equation (\refeqn{eqn:rhs}) contains the nonlinear convective term $N(\mathbf{u}_i^n)$ evaluated explicitely, the external forcing $\mathbf{f}_i$ and ???. Equation (\refeqn{eqn:hmhz_pres}) is solved using a Generalized minimal residual method (GMRES), while equation (\refeqn{eqn:hmhz_vel}) is solved using the conjugate gradient (CG) mehtod. Furthermore, both the pressure and velocity vectors are projected onto a space of previous solutions in order to speed up the convergence of the iterative solvers. Projections occur before and after the 
% iterative solver. %When solving equations (\refeqn{eqn:hmhz_pres}) and (\refeqn{eqn:hmhz_vel}), we obtain the corrections for the pressure and the velocity that we can use to update those variables in Equations \refeqn{eqn:update_p} and \refeqn{eqn:update_u}.

%\subsubsection{Coarse grid solver}

% introduce coarse grid solver for the pressure

\subsection{Implementation}
\label{sec:implementation}

\subsubsection{Mesh and mapping}
\label{sec:code}
The geometry is meshed using hexahedral elements, partitioned for parallel
computation using a spectral bisection algorithm as implemented in "genmap"
which accompanies the code Nek5000 \cite{argonne:nekdoc}. An example of the
partitioning for the case $Re_{\tau} = 180$ run on $64$ cores is shown in \reffig{fig:partition_vis}, where each element is colored according to the MPI rank it belongs to. We note that partition is done at the element level and not further.
\begin{figure}
  \centering
  \subfigure[Partition]{
  \includegraphics[trim=300 400 800 350,clip,width=\linewidth]{./figures/partition2.png}
  \label{fig:partition_vis}
  }
  \subfigure[Velocity magnitude]{
  \includegraphics[trim=300 400 800 350,clip,width=\linewidth]{./figures/vel_magn.png}
  \label{fig:flow_vis}
  }
  \caption{Partition of the elements and velocity magnitude in the pipe ($Re_{\tau}=180$).}
  \label{fig:partition}
\end{figure}
 
\subsubsection{Code structure}
\label{sec:code}

The sequence of operations leading to the solution of incompressible
Navier-Stoke is summed up algorithmically in \refalg{alg:code_struct}.  
First of all, note that both \refeqn{eqn:hmhz_pres} and \refeqn{eqn:hmhz_vel} can be summed up in discrete form as
$$Hu=(h_1 A+h_2 B)[u]$$
Different choices for the factors $h_1$ and $h_2$ yield either the Poisson equation, or the Helmholtz equation
\begin{eqnarray}
Hp &=& (Ap)=f_p, \quad (h_1=1, h_2=0)\\
Hu &=&(Au -\frac{b_0}{\Delta t}I u)=f_u, \quad (h_1=1, h_2=-\frac{b_0}{\Delta t}).
\end{eqnarray}
where $A$ is the stiffness matrix stemming from the discretization of the Laplacian and $I$ is the identity matrix. 

The first step is to compute the corresponding right hand sides and then project them onto a subspace of previous solutions (subspaces are denoted by $X$ and the size of the space is $L$). By virtue of the method of projections however we do not solve $Hp=f_p$, but rather $H\delta p=\delta f_p$, and details on the technicalities of this are abundant in \cite{Fischer1998}, the same applied to the velocity solver. 
The corrections $\delta p$ and $\delta \mathbf{u}$ are computed by solving the Helmholtz equation for the projection of the right hand side. A simplified structure for the Helmholtz solver is shown in \refalg{alg:helmholtz}. During the Helmholtz solve, the pressure is solved with the GMRES method while Conjugate Gradient(CG) is used for the velocity. The pressure solve also includes the computation of the preconditioner based on the Schwarz overlapping method and coarse grid solve, which is not the case for the velocity and constitutes an important part of the work and communication.
\begin{algorithm}
\caption{PNPN method.}
\begin{algorithmic}
\Procedure{Solver}{}
\For {k=1,...,nsteps} \Comment{$ttotal$}
\State \# \textbf{Compute Pressure}
\State $r_p \leftarrow rhs_p(\mathbf{u}, \mathbf{f})$ \Comment{$trhsp$}
\State $\delta r_p \leftarrow r_p - \text{proj}_{X_{r_p}}(r_p)$ \Comment{$tprojp1$}
\State $\delta p \leftarrow \text{Helmholtz}(H_p,\delta r_p)$ \Comment{$thmhp$}
\State $p \leftarrow p + \delta p$ \Comment{$tprojp2$}
\State $X_p \leftarrow \left\{ X_p, \text{proj}_{X_{p}}(p) \right\}$ \Comment{$tprojp2$}
\State $X_{r_p} \leftarrow \left\{ X_{r_p}, \text{proj}_{X_{r_p}}(H_p p) \right\}$ \Comment{$tprojp2$}
%\State $X_p, X_{r_p} \leftarrow \text{Update subspaces}(X_p, X_{r_p})$ 
\State \# \textbf{Compute Velocity}
\State $r_{\mathbf{u}} \leftarrow rhs_{\mathbf{u}}(p, \mathbf{u}, \mathbf{f})$ \Comment{$trhsv$}
\State $\delta r_{\mathbf{u}} \leftarrow r_{\mathbf{u}} - \text{proj}_{X_{r_{\mathbf{u}}}}(r_{\mathbf{u}})$ \Comment{$tprojv1$}
\State $\delta {\mathbf{u}} \leftarrow \text{Helmholtz}(H_{\mathbf{u}},\delta r_{\mathbf{u}})$ \Comment{$thmhv$}
\State ${\mathbf{u}} \leftarrow \mathbf{u} + \delta \mathbf{u}$ \Comment{$tprojv2$}
\State $X_{\mathbf{u}} \leftarrow \left\{ X_{\mathbf{u}}, \text{proj}_{X_{\mathbf{u}}}(\mathbf{u}) \right\}$ \Comment{$tprojv2$}
\State $X_{r_{\mathbf{u}}} \leftarrow \left\{ X_{r_{\mathbf{u}}}, \text{proj}_{X_{r_\mathbf{u}}}(H_{\mathbf{u}} \mathbf{u}) \right\}$ \Comment{$tprojv2$}
%\State $X_{\mathbf{u}}, X_{r_{\mathbf{u}}} \leftarrow \text{Update subspaces}(X_{\mathbf{u}}, X_{r_{\mathbf{u}}})$ 
\EndFor
\EndProcedure
\end{algorithmic}\label{alg:code_struct}
\caption{Main solver}
\end{algorithm}

\begin{algorithm}
\caption{Helmholtz solve.}
\begin{algorithmic}
\Procedure{Helmholtz}{$H, r$}
\If {Velocity}
\State $x \leftarrow CG(H, r)$
\ElsIf {Pressure}
\State $\left(M_0^{-1}\right)_{\text{Sch}} \leftarrow$ Overlapping Schwarz()
\State $\left(M_0^{-1}\right)_{\text{cgs}} \leftarrow$ Coarse grid solve() \Comment{$tcoarse$}
\State $M_0^{-1} \leftarrow \left(M_0^{-1}\right)_{\text{Sch}} + \left(M_0^{-1}\right)_{\text{cgs}}$
\State $x \leftarrow GMRES(M_0^{-1}, H, r)$
\EndIf
\State \textbf{return} x
\EndProcedure
\end{algorithmic}\label{alg:helmholtz}
\caption{Helmholtz solver}
\end{algorithm}

\section{Benchmarking}
\label{sec:benchmarking}

\subsection{Hardware}
\label{sec:hardware}

The test cases were run on three different supercomputers, namely Mira from the
Argonne National Laboratory, USA, Titan from the Oak Ridge National Laboratory,
USA, and Beskow from the PDC Center for High Performance Computing, KTH, Sweden.
A quick overview of the characteristics of each computer is summarized in 
\reftab{tab:computer_charac}.
\begin{table*}
\centering
\caption{Overview of the characteristics of the different supercomputers.}
\begin{tabular}{l|ccccc} 
\hline
 & Architecture & Number of cores & Cores/node & Topology & Processes/core \\
 \hline
Mira   & IBM BG/Q  & $786,432$ & $16$ & 5D torus  &2\\ 
Titan  & Cray XK7  & $299,008$ & $16$ & 3D torus  &1\\ 
Beskow & Cray XC40 & $53,632$  & $32$ & DragonFly &1\\
\hline
\end{tabular}
\label{tab:computer_charac}
\end{table*}
The systems vary from small to large petascale and are meant to establish an overview of the Nek5000 scaling. On Mira, Nek5000 achieves its
peak performance when run with two processes per BG/Q core, being 32 processes
per node which was noted already in \cite{fischer:scaling}. Although Titan is a
machine aimed at hybrid parallelism using graphics processing units (GPUs), which
Nek5000 supports marginally as mentioned in \cite{Otten2016}, no production runs
were performed outside the MPI environment and we shall restrict the use of
Titan to CPU parallelism and rely solely
on the 16 Opteron cores per node with one process each. The same setup of 1
process per CPU core was applied to the smallest system Beskow, a Cray XC40.

\begin{table}
\centering
\caption{Overview of the latencies and bandwidths.}
\begin{tabular}{l|ccccc} 
\hline
% & Lat. $\alpha^* (\unit[]{\mu s})$ & Inv. band. $\beta^* (\unit[]{\mu s/wd})$ & Inv. flop rate $t_a (\unit[]{\mu s})$ & Non-dim. lat. $\alpha$ & Non-dim.  inv. band. $\beta$ \\
 & $\alpha^* (\unit[]{\mu s})$ & $\beta^* (\unit[]{\mu s/wd})$ & $t_a (\unit[]{\mu s})$ & $\alpha$ & $\beta$ \\
 \hline
 Mira   &  $4$   & $5\cdot 10^{-3}$                     & $1.1 \cdot 10^{-3}$ & $3600$ & $5$   \\ 
Titan  & $2.25$ & $1.42\cdot 10^{-3}$ & $6.5 \cdot 10^{-4}$ & $3500$ & $2.2$ \\ % alpha*= 2.2515e-06 s, beta*= 1.4179e-09 wd/s, t_a= 6.5e-04   , alpha=alpha*/t_a, beta_beta*/t_a
Beskow & $2.55$ & $8.25\cdot 10^{-4}$ & $1.5 \cdot 10^{-4}$ &$17000$ & $5.5$ \\ % alpha*= 2.5486e-06 s, beta*= 8.2542e-10 wd/s, t_a= 1.5013e-04, alpha=alpha*/t_a, beta=beta*/t_a
\hline
\end{tabular}
\label{tab:alpha_beta}
\end{table}

In order to assess the performance of the machines at hand, we computed  some of the network characteristics that determine the communication. In particular Beskow, which is a relatively new machine, had no such parameters provided to users. 
The performance study conducted here relies on the linear interprocessor communication  model
\begin{equation}
 t_c(m) = (\alpha + \beta m) t_a,\label{eqn:lincomm}
\end{equation}
where $t_c$ is the communication time, $m$ is the message length (number of
64-bit words) and $t_a$ is the inverse of the observed flop rate. The relevant
quantities here are $\alpha$ and $\beta$, the non-dimensional latency and inverse
 bandwidth. We denote by $\alpha^*$ and $\beta^*$ the corresponding dimensional 
latency and inverse bandwidth. The relation between dimensional and non-dimensional parameters is given by
\begin{equation*}
\alpha = \frac{\alpha^*}{t_a} \qquad \text{and} \qquad \beta = \frac{\beta^*}{t_a}.
\end{equation*}
The values of $\alpha^*$ and $\beta^*$ are computed following a "ping-pong" test 
as described in \cite{fischer:scaling}. During this test, the time required to 
send and receive messages of various sizes between 512 processors is measured 
and subsequently the values of $\alpha$ and $\beta$ are computed as the best fit 
for the linear model Equation.~\refeqn{eqn:lincomm}. The value of $t_a$ is determined by 
performing a number of matrix-matrix multiplications representing the tensor products
 that are at the core of a spectral element solver \cite{fischer:hom}. The tensor
 products considered imply 3D elements with polynomial order ranging from $10$ to $13$.
 Three different interpretations of the memory layout for the matrices are considered 
leading to a total of $12$ tests. For each test, the time and number of operations are
 measured and flop rate is computed. Data are then averaged and $t_a$ is taken as the
 inverse of the mean flop rate. Results for the ping-pong test are shown in 
\reffig{fig:pingpong} along with the linear model for all computers. An overview 
of the latencies, bandwidths and inverse flop rates is presented in 
\reftab{tab:alpha_beta}. The non-dimensional parameters $\alpha$ and $\beta$ are 
a relative measure of the communication to computation cost. High value for these
parameters imply that the limitation in parallel efficiency will arise earlier due to communication. Consistent all throughout our studies is   that all machines are strongly limited by the latency, already a noted common feature of modern computers. This limitation is strongest on
Beskow, which is the newest of the three machines and has fastest CPUs. Therefore,
it is expected that Beskow will not scale as well as the two others.
% Consequently, it 
% is expected that on a computer with high values onof those parameters, like Beskow,
% the scalability limit will occur for a lower number of cores because communication
% will become a limitation earlier. It does not mean however that such a computer is
% slower in terms of absolute compute time. %For example, we expect Beskow to be faster than Mira and Titan because of its newer and faster CPUs and interconnection network, despite higher values of $\alpha$ and $\beta$.

\begin{figure}
  \centering
  \subfigure[Ping-pong on Beskow]{
  \includegraphics[width=\linewidth]{./figures/pingpong_beskow.png}
  \label{fig:pingpong_beskow}
  }
  \subfigure[Ping-pong on Titan]{
  \includegraphics[width=\linewidth]{./figures/pingpong_titan.png}
  \label{fig:pingpong_titan}
  }
  \subfigure[Ping-pong and all-reduce on Mira as illustrated in \cite{fischer:scaling}]{
  \includegraphics[width=\linewidth]{./figures/pingpong_mira.png}
  \label{fig:pingpong_mira}
  }
  \caption{Latency and Bandwidth Tests on Mira}
  \label{fig:pingpong}
\end{figure}

\subsection{Code instrumentation for profiling}
\label{sec:timers}
Our strategy for timing the code execution is twofold. We use both MPI timers
for the wall clock time and profiling libraries tailored to the machine
architecture to induce the least overhead in communication. Both the timers and
profilers are set to start counting after the initial setup stage is completed,
in our case after timestep 30, lasting for an extra 20 timesteps. The initial
stage is meant to allow for the higher order restart, proper initialization of the projection space (i.e. the size of the projection space $X$ is $L=5$, thus requiring 5 consecutive soltuions).
 
\paragraph{MPI Timers}
The timers are distributed in the code according to \ref{alg:code_struct}. Each one of them measures the wall
clock time using the MPI timer ({\tt MPI\_Wtime}) followed by a barrier
({\tt MPI\_Barrier}) guaranteeing coherent and synchronized measurements. We also performed a reference run without synchronization and timers to evaluate the overhead due to profiling. In all instances this was far
below 5\%. 

Following \ref{alg:code_struct} the main timer ($ttotal$) measures the total
time spent in the outer timestepping loop for the last 20 timesteps. Additional timers for the computational cost of the right hand side Equation.~\refeqn{eqn:hmhz_pres} ($trhsp$), and of the right hand side of Equation.~\refeqn{eqn:hmhz_vel} ($trhsv$). We also time all the different projections separately, using two timers for both the pressure and velocity: one timer for projecting the residuals before the Helmholtz solve ($tprojp1$ and $tprojv1$) and one for updating and projecting the field after the solve ($tprojp2$ and $tprojv2$), this second projection step also includes the time to update the subspaces. The time spent in the Helmholtz solves is measured for pressure as well as velocity ($thmhp$ and $thmhv$). Within the pressure solver, a timer wraps around the coarse grid solve ($tcoarse$) since we know that this particular section is bound by communication. Finally, we gather in an additional timer most of the remaining computations such that our timers account for more that 95\% of the total time. 
%We learn from these timers that the projections are in most cases the most time consuming functions, followed by the Helmholtz solves. 

Since these timers do not provide information about the distribution between time spent in computation and communication, which is the core element when assessing scalability and efficiency of a parallel code we shall not focus further on them. Instead we investigate in detail  the timings produced by more adapted sampling and profiling tools.
\paragraph{Craypat and Hardware Performance Monitor}
In order to measure the time spent in communication we relied on Craypat for
Beskow and Titan and on Hardware Performance Monitor (HPM) for Mira. Both tools
allow us to measure the total time spent in communication during the targeted 20 time
steps. HPM gives additional information on the cache misses and the load
imbalance. The CrayPat performance analysis framework is used to sample the code during execution at a default frequency of $\unit[100]{Hz}$ and reports in which function each sample was taken. Then, we assume that the proportion of the total time spent in a given function is equal to the proportion of samples within this function. The sampling procedure ensures a very low overhead. We also tested the profiling procedure, where all function calls are traced, available with Craypat but overhead in time was about $50\%$ and the method was abandoned.
\subsection{Test Case: pipe flow}
\label{sec:pipe}

The test case considered is the turbulent flow in a straight pipe. A thorough description of the flow configuration as well as a detailed analysis of the physical results can be found in \cite{Khoury2013}. The flow was run at four different friction Reynolds numbers $Re_{\tau} = 180$, $360$, $550$ and $1000$. A summary of the different simulations and associated number of elements, polynomial order $N$ and number of grid points $n$ is presented in \reftab{tab:pipe_conf}. The friction Reynolds number is defined as $Re_{\tau} = u_{\tau} R / \nu$, where $u_{\tau}$ is the friction velocity, $R$ is the radius of the pipe and $\nu$ is the kinematic viscosity. The bulk Reynolds number is defined as $Re_{b} = 2 U_b R / \nu$, where $U_b$ is the mean bulk velocity. A snapshot of the velocity magnitude for the case $Re_{\tau} = 180$ is illustrated in \reffig{fig:flow_vis}.

\begin{table}
\centering
\caption{Summary of the different pipe flows configurations.}
\begin{tabular}{llrrr} 
\hline
$Re_{\tau}$&$Re_{b}$&\# of elements & $N$ & \# of grid points\\ 
\hline
$180$ & $5300$ & $36,480$ & $8$ & $18.67 \times 10^6$\\
$360$ & $11,700$ & $237,120$ & $8$ & $121.4 \times 10^6$\\ 
$550$ & $19,000$ & $823,632$ & $8$ & $437.0 \times 10^6$\\ 
$1000$ & $37,700$ & $1,264,032$ & $12$ & $2.184 \times 10^9$\\
\hline
\end{tabular}
\label{tab:pipe_conf}
\end{table}
%\subsection{Abstractions and Assumptions}
%\label{sec:abstractions}
%\subsubsection{$\alpha$, $\beta$}
%\subsubsection{Imbalance}
%\subsubsection{Weak/Strong Scaling, Efficiency}
%\subsubsection{Cache Misses and Scaling}
%\subsubsection{Partitioning and Imbalance}


%\subsection{Models for parallel performance}

%\subsubsection{Computational complexity}

%\subsubsection{Communication model}

% In practice, each simulation is restarted from a previously computed turbulent solution and is run during $50$ time steps. The projections for velocity and pressure are turned on after $5$ time steps, the number of the previous pressure solutions saved is $20$ and the timers are turned on during the last $20$ time steps. Therefore, heavy input/output is not included and projections are working fully during the measurement period.

% Present system of equations
% Describe briefly the algorithm behind nek
% Mention and discuss PN-PN
% Discuss the two coarse grid solver XXT and AMG

\section{Performance and Scaling Analysis}
\label{sec:analysis}
Our abstraction assumes that large scale runtime performance is mainly composed
of
\begin{enumerate}
  \item system hardware parameters consisting of the network topology, latency and
    bandwidth\label{enum:a}, and CPU flops per second, 
  \item time $T_a$ and $T_c$ spent in computation and communication largely
    dependent on \ref{enum:a}. and on their respective algorithmic complexities,
  \item and partitioning.\label{enum:c}
\end{enumerate}

The partitioning for our test case (see \refsec{sec:pipe}) is considered to be
topologically equivalent to a cube. The resulting runtime complexities are
extensively described in \cite{fischer:scaling} for the Mira system. Based on
these theoretical results we use profiling tools and wall clock timers to
measure the load imbalance,
cache misses, as well as weak and strong scaling. Load imbalance and cache
misses are only measured on Mira via the HPM profiling library. 
We want in particular to verify
experimentally the strong scaling limit for a given problem of size $N$. 
In this paper, the strong scaling limit for a problem of size $N$ is defined
as the minimal number of grid points per process $\frac{N}{P}$ by finding $P$ such that

\begin{equation}
  \dfrac{T_a(N,P)}{T_c(N,P)}=1,\mbox{ for problem size }N.
  \label{eq:strong}
\end{equation} 

\noindent Alternatively, the strong scaling limit is commonly described by the
derivative $\frac{dT}{dP}$ of the
of the total time $T(N,P)$ being equal to $0$. So the point where the runtime
starts increasing again with increasing $P$. That point represents the fastest
time to solution. This is rather an upper bound of the strong scaling
limit that would in most of our test runs never be observed. Hence, it has
little practical meaning to the user, as he would never run the code in that
regime, either due to the granularity limit or due to high computational costs. \refeqn{eq:strong} gives a lower bound and much more practical notion of the strong
scaling limit. If communication takes longer than
computation, the code is deemed to be at the strong scaling limit where the
scaling starts diverging significantly from the perfect linear scaling. 

A generic case, widely known across the CFD community, is used to explore the
scaling behavior of Nek. This should allow potential users to estimate and
compare the scaling of Nek to other CFD software.
Our test case is run in four different regimes at 4 different problem sizes called
$Re_{\tau} = 180$, $360$, $550$ and $1000$ described more in
detail in \refsec{sec:pipe}.

\section{Results}

Our four test cases were run with various processor counts on three systems. The
lower bound for the processor count is given by the amount of RAM to fit a given
problem into memory. Nek5000 has roughly a memory requirement of 500 fields
times the number of degrees of freedom. The upper bound was either set by the
administrative limit of getting access to the maximum number of processors
(Beskow and Titan) or by the algorithmic limit of having at least one element per process. As has
been pointed out in \refsec{sec:code}, the parallelization of Nek5000 is at the
element level so that one element may not be partitioned further. 
As anticipated in \refsec{sec:timers}, we measure the communication time and
computation time of the chosen 20 timesteps. 

In \cite{fischer:scaling} the strong scaling limit \refeqn{eq:strong} was estimated
to be at about $\frac{N}{P}=2000$ for the conjugate gradient (CG) and $20000$ or
$7000$ (BG/Q) for the algebraic multigrid (AMG). As described in \refsec{sec:code}, the
Nek5000 solver using the PNPN method consists of several CG, GMRES and XXT or AMG solves. Furthermore,
there is an overhead for the setup, the projections, the coarse solve and other
negligible runtime components. Moreover, the convergence of the underlying
algorithms is highly case dependent, thus influencing the overall runtime
behavior. 

\subsection{Scaling plots}

The scaling plots for all three
systems in \reffig{fig:scaling_mira}, \reffig{fig:scaling_beskow} and \reffig{fig:scaling_titan} are the fundamental
measurements for our further analysis of the weak/strong scaling. They show the compute time and
communication time as well as the total time for both XXT and AMG over all our
test cases and computer systems. They are all done in logscale because we were
doubling the number of processes $P$. However, in order to help identify linear scaling of the
compute time, the optimal linear scaling line of the compute time was added. We
do not compare to linear scaling of the total time as we operate in the strong
scaling limit regime, where parallel efficiency is supposed to be well below $1$.

\begin{figure}
  \centering
  \subfigure[$Re_{\tau} = 180$]{
  \includegraphics[width=0.85\linewidth,height=0.2\textheight]{./figures/mira/MiraReTau180.eps}
  }
  \subfigure[$Re_{\tau} = 360$]{
  \includegraphics[width=0.85\linewidth,height=0.2\textheight]{./figures/mira/MiraReTau360.eps}
  }
  \subfigure[$Re_{\tau} = 550$]{
  \includegraphics[width=0.85\linewidth,height=0.2\textheight]{./figures/mira/MiraReTau550.eps}
  }
  \subfigure[$Re_{\tau} = 1000$]{
  \includegraphics[width=0.85\linewidth,height=0.2\textheight]{./figures/mira/MiraReTau1000.eps}
  }
  \caption{BG/Q Mira times: AMG ({\color{blue}blue}), XXT ({\color{red}red}), communication (dashed),
  computation (single), communication (ticked), computational linear scaling
  ({\color{green}green})}
  \label{fig:scaling_mira}
\end{figure}

\begin{figure}
  \centering
  \subfigure[$Re_{\tau} = 180$]{
  \includegraphics[width=0.85\linewidth,height=0.2\textheight]{./figures/beskow/BeskowReTau180.eps}
  }
  \subfigure[$Re_{\tau} = 360$]{
  \includegraphics[width=0.85\linewidth,height=0.2\textheight]{./figures/beskow/BeskowReTau360.eps}
  }
  \subfigure[$Re_{\tau} = 550$]{
  \includegraphics[width=0.85\linewidth,height=0.2\textheight]{./figures/beskow/BeskowReTau550.eps}
  }
  \subfigure[$Re_{\tau} = 1000$]{
  \includegraphics[width=0.85\linewidth,height=0.2\textheight]{./figures/beskow/BeskowReTau180.eps}
  }
\caption{Beskow: AMG ({\color{blue}blue}), XXT ({\color{red}red}), communication (dashed),
  computation (single), communication (ticked), computational linear scaling
  ({\color{green}green})}
\label{fig:scaling_beskow}
\end{figure}

\begin{figure}
  \centering
  \subfigure[$Re_{\tau} = 180$]{
  \includegraphics[width=0.85\linewidth,height=0.2\textheight]{./figures/titan/TitanReTau180.eps}
  }
  \subfigure[$Re_{\tau} = 360$]{
  \includegraphics[width=0.85\linewidth,height=0.2\textheight]{./figures/titan/TitanReTau180.eps}
  }
  \subfigure[$Re_{\tau} = 550$]{
  \includegraphics[width=0.85\linewidth,height=0.2\textheight]{./figures/titan/TitanReTau180.eps}
  }
  \subfigure[$Re_{\tau} = 1000$]{
  \includegraphics[width=0.85\linewidth,height=0.2\textheight]{./figures/titan/TitanReTau180.eps}
  }
\caption{Titan: AMG ({\color{blue}blue}), XXT ({\color{red}red}), communication (dashed),
  computation (single), communication (ticked), computational linear scaling
  ({\color{green}green})}
\label{fig:scaling_titan}
\end{figure}



%\subsection{Weak and Strong Scaling}

\subsection{Strong scaling limit}

For both XXT and
AMG the strong scaling limit is defined graphically in the plots by the
intersection of the computation time
and the communication time. The approximations for minimal values of
$\frac{N}{P}$ were determined using linear interpolation (see
\reftab{tab:stronglimit}).

\begin{table}  
  \caption{Strong Scaling Limit for all 4 test cases on Beskow, Titan and Mira
  in degrees of freedom per core $\frac{N}{P}$ with 2 processes per core on Mira, 1 process per core on Beskow and Titan}
  \centering
  \begin{tabular}{c||cccc}
    \hline
    \hline
    {\bf Mira}
    &$Re_{\tau} 180$&$Re_{\tau} 360$&$Re_{\tau} 550$&$Re_{\tau} 1000$\\
    \hline
    XXT&4496&3412&4192&\\
    AMG&5040&5578&6200&9750\\
    \hline
    \hline
    {\bf Beskow}
    &$Re_{\tau} 180$&$Re_{\tau} 360$&$Re_{\tau} 550$&$Re_{\tau} 1000$\\
    \hline
    XXT&45700&19200&26000& - \\
    AMG&24800&33000&48000& - \\
    \hline
    \hline
    {\bf Titan}
    &$Re_{\tau} 180$&$Re_{\tau} 360$&$Re_{\tau} 550$&$Re_{\tau} 1000$\\
    \hline
    XXT&9000&24000&65000&228000\\
    AMG&11000&36000&68000&132000\\
    \hline
    \hline
  \end{tabular}
  \label{tab:stronglimit}
\end{table}

% 

In practice we observed a strong scaling limit for XXT at roughly $1900<
\frac{N}{P} < 2300$ and for AMG at $2300<\frac{N}{P}<4000$ on Mira, below the $7000$
anticipated in \cite{fischer:scaling}. On Beskow, the scalability limit is 
more delicate to locate accurately because 
of the high variance in communication times (see \reffig{fig:pingpong_beskow}), 
especially for the small cases. We nevertheless present the scalability limit for 
the particular results from \reffig{fig:scaling_beskow} while keeping in mind that
this is just the result for one run and not an average value. We note that
the scalability limit on Beskow is roughly one order of magnitude higher than on 
Mira in terms of degrees of freedom per core and is located at about 
$\frac{N}{P} \sim 20000 - 50000$. This is consistent with the values for the
non-dimensional latency $\alpha$ %and inverse bandwidth $\beta$ 
from Table \reffig{tab:alpha_beta} 
and the fact that Beskow has faster CPUs, thus having an increased value for
$t_a$ that leads to a higher $\alpha$, although $\alpha*$ are comparable for
Titan, Mira and Beskow. 
As an intermediate conclusion, we see that the scalability limit on Mira and Beskow
is almost independent on the problem size and number of cores that are used.  On
Titan, the scalability limit exhibits a different behavior.
The strong scaling limit for $\frac{N}{P}$ increases
significantly with bigger cases as we see from Table \reffig{tab:stronglimit}.
The limit goes from $\frac{N}{P} \sim 9000$ to $\frac{N}{P} \sim 228000$ between
the cases $Re_{\tau}=180$ and $Re_{\tau}=1000$ for XXT. It goes from
$\frac{N}{P} 
\sim 11000$ to $\frac{N}{P} \sim 132000$ between the cases $Re_{\tau}=180$ and 
$Re_{\tau}=1000$ for AMG. This limitation of the weak scaling cannot be explained 
from the non-dimensional latencies and bandwidths from \reftab{tab:alpha_beta}. 
Looking at the non-dimensional latency $\alpha$ we do expect Titan to have similar values as Mira. 
In practice, we observe a similar strong scaling limit for the case $Re_{\tau} = 180$
but a far lower strong scaling limit for the bigger cases. 

% First explanation
%The only 
The first plausible explanation to us is the occasional, random latency spikes seen in the ping-pong tests. If
one process experiences this, the created imbalance has repercussion for all
processes in a parallelized CFD code. We see that these spikes increase communication by
an order of magnitude or higher. The more compute nodes are involved, the higher
the risk of a latency spike. This may partially explain why Titan's strong
scaling limit in grid points per process $\frac{N}{P}$ seems to increase with increasing $P$.

% Second explanation
A second possible reason is the fact that the ping-pong test used to compute the parameters $\alpha$ and $\beta$ is performed on $512$ 
cores only. During this test, the cores are very likely to lie close to each other on the computer. 
However, this ideal situation does not hold any more when a high number of cores
is considered as they are probably split on many remote nodes.
A poor interconnect network between cabinets or a high network load on Titan 
could be the explanation for the observed deterioration and the values for 
$\alpha$ and $\beta$ from \reftab{tab:alpha_beta} become irrelevant on Titan 
when the number of cores is too high.


\subsection{Super Linearity}
% Superlinearity

In theory the compute time scaling should exactly match linear scaling
as computational work is distributed according to the ratio $\frac{N}{P}$. All
algorithms in Nek5000 should yield little computational overhead due to the halo update.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./figures/cachearrow.eps}
  \caption{Cache misses on Mira for all 4 test cases(low to high degrees of freedom in direction of the arrow) The dashed red line: L1 cache size}
   \label{fig:cachemisses}
\end{figure}

All test cases on all the systems show a super linear scaling. This
observation holds true with all timers and profilers switched off.
Numerically there is no explanation for this behavior. 

The usual explanation for super linear scaling is a sudden decrease of the cache
misses for decreasing $\frac{N}{P}$, as parts of the solver can entirely work on
data that lies in the cache. 
To investigate this, we extracted the cache misses on Mira provided by HPM (see
\reffig{fig:cachemisses}). Mira is equipped with L1 cache of 16kb. The L2 cache
of 32mb, that is located at the node level, is irrelevant as we had consistently
over 97\% cache misses. The L1 cache can be filled with $2048$ double precision
numbers, see
\reffig{fig:cachemisses} the vertical dashed red line which indicates the cache 
size below which all gridpoints would theoretically fit into cache. As we run with 2 processes per core, this would be at roughly 1000
degrees of freedom per process. Although the data fields achieve those sizes only at the strong scaling
limit we do observe a general decrease in cache misses with decreasing
$\frac{N}{P}$. In summary, we attribute the super linear scaling to cache
management and pipelining on the CPU of Mira. We do not possess profiling results 
for Beskow or Titan and cannot study the cache misses there but we assume that the 
reason for super linearity is similar as for Mira.


\subsection{Comparison between XXT and AMG}
% XXT AMG comparison

Comparing XXT and AMG we have a very clear picture on Mira. For smaller element
numbers ($Re_{\tau}=180$) XXT performs best. From 200k elements onwards for Mira
($Re_{\tau}=360$, $550$ and $1000$) AMG takes the lead. Although AMG has a %much 
lower strong scaling limit, it outperforms XXT in time to solution (e.g. $Re_{\tau} = 
550$). This difference is marginal on Titan and visible on Beskow for $Re_{\tau} = 
550$ and $1000$ without being as pronounced as on Mira. 
However, we can claim that using AMG on these
machines is never detrimental with regard to time to solution. Therefore, when taking
all the machines into account, we see that AMG generally leaps ahead if the
systems rely on a low latency network combined with a high element count. In
\reftab{tab:xxtamg} we compare XXT and AMG on $Re_{\tau} = 550$ at the strong
scaling limit of AMG (4096 nodes). The amount of data communicated by XXT is by
an order of magnitude higher, while AMG uses twice as many MPI calls. 
\begin{table}
\caption{Number of MPI calls and data communicated on $P=131,072$ at $Re_{\tau}=
550$}
\centering
\begin{tabular}{l||c|c||c|c}
\hline
&\multicolumn{2} {c||} {\bf AMG}&\multicolumn{2} {c} {\bf XXT}\\
\hline
MPI Routine   &  \#calls  &  bytes  &   \#calls  & bytes \\ 
\hline
MPI\_Isend     &  96638   &       360.5   &  62336     &    6363.8    \\     
MPI\_Irecv     &  96638   &       362.7   &    5916    &    61885.4   \\   
MPI\_Waitall   &  38971   &         0.0   &   56420    &      542.0   \\     
MPI\_Allreduce &  10956   &      5921.0   &    9082    &        0.0   \\     
\hline
Total&252312&&136272\\                                                 
\hline
\end{tabular}
\label{tab:xxtamg}
\end{table}

\subsection{Load imbalance}
% Load imbalance

Beyond the strong scaling limit, the computation time increases and
approaches the linear scaling line again. This is attributed to the load
imbalance as in the extreme case some processes have to work on one element and
some processes on two elements. This can be observed in
\reffig{fig:imbalancehist}
where the imbalance for $Re_{\tau}=1000$ on 32768 nodes creates two spikes in the
distribution of the execution time. The histogram includes the imbalance of
the workload as well as the resulting imbalance in the communication.
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./figures/loadbalance.eps}
  \caption{Histogram of the load imbalance at $2.184\times10^9$ degrees of freedom: number of processes per time spent in communication. Red bins:\% of processes at the lowest per core load, blue bins:\% of processes at highest per core load}
  \label{fig:imbalancehist}
\end{figure}

\subsection{Weak scaling}
% Weak scaling

We can investigate the weak scaling on Mira a bit more in details. On this computer, the communication is mostly latency bounded with a small influence of the
bandwidth. This holds true for peer to peer communication as well as for the
all-reduce (see \reffig{fig:pingpong}). Across the four test cases we can observe a weak scaling in
\reffig{fig:weakscaling}. It proves that the scaling on Mira is only dependent
on the ratio $\frac{N}{P}$. 
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./figures/weak.png}
  \caption{Weak Scaling}
  \label{fig:weakscaling}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./figures/projections.eps}
  \caption{Convergence of solution with increase of projections space $L$. (green: $L=20$, red $L=0$) }
  \label{fig:projections}
\end{figure}


\section{Conclusion}
Our four test cases from 18 million to 2 billion degrees of freedom were
successfully run on three different petascale system architectures from the
lowest, memory bound, processor count to the granularity limit in order to
assess the strong scaling of Nek5000. On Mira we can confirm the theoretical
limits established in \cite{fischer:scaling} to match our results in the order
of magnitude. On the Cray systems we observed one to two orders of magnitude
lower strong scaling limits due to by an equally order of magnitude higher
latency and high noise in the network communication. We investigated the new
and updated projections and observed a much better behavior compared to past
implementations. Our results point at the regimes under which to chose AMG or
XXT; AMG for low latency and high element count, XXT for high latency, high
bandwidth and low element count. Our model failed to explain the difference in
the strong scaling limit between Titan and Mira. A model that would include a
quantity for the network noise would improve our prediction on noisy systems.
We confirmed that a synchronized and low latency global communication remains
crucial for strong scaling of a spectral element based CFD solver.

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
We thank Scott Parker and Kevin Harms from ALCF at Argonne their invaluable
suggestions and insights to the performance analysis on Mira. 
%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{easc2016}  % template.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
%\appendix
%Appendix A
%\section{Appendix A}
%\label{sec:plots}
% This next section command marks the start of
% Appendix B, and does not continue the present hierarchy

%\balancecolumns % GM June 2007
% That's all folks!
\vskip 40pt
\begin{flushright}
\scriptsize \framebox{\parbox{3.2in}{
The submitted manuscript has been created by the University of Chicago
as Operator of Argonne National Laboratory (``Argonne'') under
Contract No. DE-AC02-06CH11357 with the U.S. Department of Energy.
The U.S. Government retains for itself, and others acting on its
behalf, a paid-up, nonexclusive, irrevocable worldwide license in said
article to reproduce, prepare derivative works, distribute copies to
the public, and perform publicly and display publicly, by or on behalf
of the Government.}} \normalsize
\end{flushright}
\end{document}

